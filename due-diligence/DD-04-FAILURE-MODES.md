# DD-04 — Failure Modes and Recovery

---

## 1. Scope

This document examines what happens when components of the Yggdrasil system fail. It covers infrastructure failures, data integrity failures, and trust failures between organizations. For each failure mode, it describes the system's behavior, what is preserved, what is lost, and what manual intervention is required to recover.

The system is in alpha. Several of the recovery mechanisms described here are designed but not battle-tested under production load. Where a mechanism exists only as a design or configuration option rather than as tested code, this document says so.

## 2. Coordination VPS Failure

The central VPS hosts the coordination infrastructure — the Redpanda event broker, Go mesh server, customer portal, and Caddy reverse proxy. It does not host tenant ERP servers or databases. Each tenant's C++ server and PostgreSQL database run locally on tenant hardware.

If the VPS goes offline, tenant ERP systems continue operating normally. Desktop clients and web browsers connect to the local C++ server; all CRUD operations, reporting, inventory management, and internal workflows proceed without interruption. What fails is the coordination layer: cross-tenant B2B event federation stops, the customer portal becomes unreachable, and the mesh server's health monitoring and API proxying halt.

Events generated by tenants during the outage are persisted to their local `b2b_events` table with status `pending`. The Go sidecar enters a reconnection loop with exponential backoff. When the VPS recovers and the Redpanda broker comes back online, sidecars reconnect and pending events are published. Events destined for tenants during the outage accumulate in the broker once it recovers and the source tenant's sidecar flushes its queue.

If the VPS itself is unrecoverable — disk failure, hardware loss — the Docker Compose configuration, Cloudflare tunnel token, and coordination database (`ygg_central`) must be deployed to a new host. The daily backup script (`backup-db.sh`, 03:00 UTC, 14-day retention) covers the mesh database. RPO for coordination metadata is one day. RTO is estimated at one to two hours for manual recovery, including provisioning a new VPS, restoring the mesh database backup, and re-establishing the Cloudflare tunnel. This has not been tested.

The concentration of all coordination services on a single VPS is a known architectural limitation. Mimir Labs is pursuing partnerships with Fastly and/or Cloudflare for edge-level redundancy and CDN-based failover, which would distribute the coordination layer across multiple points of presence and eliminate the single-VPS dependency for the management plane.

## 3. Tenant Local Infrastructure Failure

Each tenant's C++ server and PostgreSQL database run on the tenant's own hardware. If the tenant's database crashes or its storage is corrupted, that tenant's ERP functionality stops. Other tenants are unaffected — there is no shared database infrastructure between tenants.

The C++ server includes graceful degradation for the in-memory cache — a cache hit can serve stale data for read requests when the database is temporarily unreachable. But writes fail immediately, and once the cache TTL expires, reads fail too. There is no offline mode.

Recovery depends on the tenant's backup procedures. The system provides `pg_dump`-based backup tooling, but the tenant (or the provider on their behalf for managed deployments) is responsible for scheduling and retention. Any changes made between the last backup and the failure are lost from both the business data and the audit trail. There is no write-ahead log replication to a secondary database — this is a planned Phase 3 enhancement (PostgreSQL streaming replication), not a current capability.

The audit trail survives to the extent that the backup captures it. Since the `audit_change_log` table lives in the same database as the business data, it is backed up and restored together. Database-level triggers (migration 014) ensure that once restored, audit records cannot be modified or deleted.

B2B events that were in transit to or from the failed tenant at the time of failure remain in the Redpanda broker for up to 7 days. Once the tenant's database is restored and the C++ server restarts, the sidecar reconnects and the RedpandaRelay resumes consuming events from the broker. Events older than 7 days at the time of restoration are lost from the broker. The `b2b_events` table on the partner tenant's side retains the outbound event records regardless of broker retention.

## 4. Connectivity Failure

Tenant ERP systems connect to the coordination VPS through Cloudflare Zero Trust tunnels. If a tenant's tunnel drops while their local infrastructure remains healthy, the tenant's ERP continues operating in standalone mode — all local operations proceed normally. What stops is cross-tenant B2B event propagation: outbound events queue locally in the `b2b_events` table, and inbound events accumulate in the Redpanda broker.

When the tunnel reconnects, the sidecar's consumer picks up from its last committed offset and processes the backlog. If the failure extends beyond the broker's 7-day retention, events published during the gap are lost from the broker. The partner tenant's `b2b_events` table retains records of what was published, so manual reconciliation is possible but not automated.

If the Cloudflare tunnel between the VPS and the internet drops (affecting the VPS itself), the impact is the same as a VPS failure: all tenants lose coordination and B2B federation, but their local ERP operations continue uninterrupted. Mimir Labs is exploring edge CDN partnerships with Fastly and/or Cloudflare to provide redundant ingress paths and reduce single-provider dependency for the coordination layer.

## 5. Partial Write and Transaction Integrity

Within a single tenant, the C++ server uses Qt's QSqlQuery within explicit transaction blocks (BEGIN/COMMIT/ROLLBACK) for operations that span multiple tables. Creating a sales order with line items, for example, inserts into `crm_sales_orders` and `crm_sales_order_lines` within a single transaction. If the connection drops mid-transaction, PostgreSQL rolls back the incomplete changes.

The audit trail entry is written as part of the same logical operation but is not wrapped in the same database transaction in all cases. A failure between the business data write and the audit write could result in a business record change that is not captured in the audit trail. This is a correctness gap. Migration 014 addressed the immutability half of SOC 2 Control 1.1 — database triggers now prevent any UPDATE, DELETE, or TRUNCATE on the `audit_change_log` table, ensuring that once an audit record is written it cannot be altered. However, the capture half remains application-level: audit writes are performed by the C++ server, not by PostgreSQL row-level triggers on the business tables themselves. Moving audit capture to database triggers would make it transactionally bound to the data change, eliminating the window where a write could succeed without a corresponding audit entry. This remains a planned improvement.

Cross-tenant writes are not transactional. When Tenant A publishes a B2B event that triggers a purchase order creation on Tenant B, these are two independent database transactions on two separate PostgreSQL instances on separate hardware. If Tenant A's event is published but Tenant B's PO creation fails, the system is in an inconsistent state: Tenant A believes the order was communicated, but Tenant B has no record of it. The event remains in Redpanda (or in the `b2b_events` table) and will be retried, but there is no two-phase commit or saga pattern that guarantees atomicity across tenants. This is a deliberate design choice — distributed transactions across organizational boundaries would require a central coordinator, which contradicts the system's custody model. The trade-off is eventual consistency with manual reconciliation as a fallback.

## 6. Event Ordering and Duplication

Within a single Redpanda topic partition, events are ordered by publication time. The system assigns one topic per module (e.g., `ygg.events.sales` for all sales events across all tenants). Topics have 3 partitions. Since events from different tenants may land in different partitions, cross-tenant event ordering is not strictly guaranteed. Two events from Tenant A to Tenant B that land in different partitions may be consumed in a different order than they were produced.

In practice, this matters most for status transition sequences. If a sales order is confirmed and then shipped in rapid succession, and the two events land in different partitions, the buyer's system might process the shipment event before the confirmation event. The system does not currently include sequence numbers or vector clocks on B2B events. Each event carries a timestamp, but the receiving system does not enforce ordering based on it.

Event duplication is possible in at-least-once delivery scenarios: a producer retries after a timeout, or a consumer processes an event but fails to commit the offset before crashing. The system does not implement idempotency keys on the consumer side for B2B events. A duplicate event could result in a duplicate purchase order on the receiving tenant. This is identified as an area for improvement but is not currently addressed.

## 7. Application-Level Failures

If the C++ server process crashes (segfault, out-of-memory, unhandled exception), the WebSocket connections to all local clients drop immediately. The desktop client's WebSocket client enters reconnection with exponential backoff. The web application's `useWebSocket` hook does the same (1s to 30s maximum backoff). When the server restarts, clients reconnect automatically.

In-memory state is lost on server crash: the CacheManager's contents (up to 512 MB), the RateLimiter's bucket state, and the MetricsCollector's counters. The cache rebuilds lazily on subsequent reads. Rate limiter state resets (briefly allowing higher-than-configured request rates). Metrics reset to zero. None of this affects data integrity — all persistent state is in PostgreSQL.

The Go mesh server, if it crashes, is restarted by Docker's restart policy on the VPS. During the outage, the portal cannot proxy requests to tenant servers, and health monitoring pauses. Tenant C++ servers and databases on tenant hardware continue operating — the mesh server's absence affects only the portal and management plane, not the tenant data plane.

## 8. Trust Failures

Trust failures are scenarios where the technical infrastructure works correctly but the business relationship between tenants breaks down.

**Disputed transaction state.** Tenant A and Tenant B disagree about the contents or timing of a transaction. The system's response is evidentiary, not adjudicative. Each tenant's audit trail provides an independent record of what their system recorded and when. The `b2b_events` table on each side provides a record of what was communicated. If the event is still within the Redpanda retention window, the broker provides a third-party record. The system does not include a dispute resolution mechanism — the SaaS contract (Article XIII, Section 13.2) specifies escalation to senior management followed by binding arbitration through the American Arbitration Association if unresolved within 30 days.

**Unilateral record modification.** One tenant modifies records related to a cross-tenant workflow — changing a quantity, voiding an order, altering a price. The modification is recorded in that tenant's audit trail. The partner tenant's records are not automatically updated (events are advisory; the receiving system decides how to process them). If a tenant modifies an order after the B2B event was sent, the partner tenant's PO retains the original values unless a subsequent update event is published. The divergence is detectable by comparing the two sides' records, but the system does not automatically detect or flag it.

**Tenant departure.** When a tenant leaves the system, their data export is governed by the SaaS contract (Article XII and Article VI, Section 6.5). The departing tenant receives their data in standard format. Their trading partners lose the real-time event stream — orders revert to manual processes (email, EDI, phone). The partner's existing records (POs linked to the departed tenant's SOs) remain intact with the B2B linkage fields (`b2b_source_tenant`, `b2b_source_order_id`) preserved but pointing to a now-inactive tenant. There is no cascade deletion of cross-tenant references; the data remains as historical record.

**Malicious tenant.** A tenant that deliberately publishes false B2B events (e.g., fabricating orders, sending spurious shipment confirmations) can cause the partner to create incorrect records. The partner's server validates incoming events against the partner registry (the mesh server's `mesh_partners` table) to ensure the source is a recognized trading partner, but it does not validate the content of the event against external sources. Content validation is the receiving tenant's responsibility — the system's workflow engine can be configured to require human approval before auto-creating records from inbound events, but this is configuration, not default behavior.

## 9. What Cannot Be Recovered

Some losses are irreversible in the current architecture.

Audit log entries are lost if the tenant database is restored from a backup that precedes the entries in question and no off-site replica exists. Since the audit log lives in the same database as the business data, a database restore rewinds both.

Events that exceed the Redpanda retention window (7 days) and were not consumed by the destination tenant are permanently lost from the broker. They exist in the source tenant's `b2b_events` table but must be manually replayed if the destination tenant needs them.

The in-memory fallback DAL in the portal (the `globalThis` Maps in the `mimirlabs` repository's `dal.ts`) loses all data on process restart — organizations, users, subscriptions, tickets, everything. This is documented as the single most critical production-blocking issue in the feature audit. Until the portal's DAL is migrated to the Prisma/PostgreSQL implementation, any portal restart in the managed SaaS environment destroys coordination metadata. The ERP data on tenant servers is unaffected, but the portal's record of tenants, users, and subscriptions must be re-seeded.

---

*Document version: 1.2 — February 2026*
*System version: Yggdrasil v0.4.4a (alpha)*
